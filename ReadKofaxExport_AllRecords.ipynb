{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ KOFAX EXPORT DATA (DREAM)\n",
    "\n",
    "Author:   Andreas Barth, SF6-S-OG  \n",
    "Version:  4, 12.10.2020  \n",
    "\n",
    "Purpose:  Script to read the email informations (doctypes, body, subject, batchID, pagecount, etc.) out of Export Scripts (File Structure) provided by KOFAX export.\n",
    "\n",
    "\n",
    "ChangeLog:\n",
    "V4: Add Reading of new Indexinformation (\"AutoClassification\")\n",
    "V3: Change Reading-Process: read Indexfile using List-Comprehensions instead of Regex.\n",
    "\n",
    "Platform:   HSDAP ONLY! ... (Raw Data is not anonymized yet)\n",
    "\n",
    "\n",
    "Steps:\n",
    "  \n",
    "0. Imports & Functions\n",
    "1. Execute Reading from File Structure\n",
    "2. Blacklist Check, Cleaning, Anonymization & Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.7 (default, May  7 2020, 21:25:33) \n",
      "[GCC 7.3.0] utf-8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/q506010/1_Kofax_Prod/Learning3010'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os, re, string, sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# ==================== BMW-Bank Funktionen ============================\n",
    "workDir = os.getcwd()\n",
    "os.chdir('/home/q506010/0_Packages')\n",
    "from BmwBankTools.downloadTools import * \n",
    "from BmwBankTools.cleanEmails import * \n",
    "os.chdir(workDir)\n",
    "\n",
    "# ==========================================================\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "np.random.seed(4711)\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "plt.style.use('ggplot')\n",
    "#===========================================================\n",
    "print(sys.version, sys.getdefaultencoding())\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 1. Read KOFAX exported data from file structure on disk (HSDAP)\n",
    "#### 1.1 Create Listing with filepath of each file to be read into system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Learning2509',\n",
       " '.ipynb_checkpoints',\n",
       " 'Recognition Time and Effectiven.csv',\n",
       " 'zipfiles',\n",
       " 'Learning0910',\n",
       " 'Learning1610',\n",
       " 'Learning2310',\n",
       " 'Learning3010']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/home/q506010/1_Kofax_Prod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Files:  39392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/q506010/1_Kofax_Prod/Learning3010/Banking/759575/2043206/001F2D46.txt',\n",
       " '/home/q506010/1_Kofax_Prod/Learning3010/Banking/759575/2043206/001F2D46_index.txt',\n",
       " '/home/q506010/1_Kofax_Prod/Learning3010/Banking/759575/2043206/.ipynb_checkpoints/001F2D46-checkpoint.txt',\n",
       " '/home/q506010/1_Kofax_Prod/Learning3010/Banking/759575/2043206/.ipynb_checkpoints/001F2D46_index-checkpoint.txt',\n",
       " '/home/q506010/1_Kofax_Prod/Learning3010/Banking/759588/2043235/001F2D63.txt',\n",
       " '/home/q506010/1_Kofax_Prod/Learning3010/Banking/759588/2043235/001F2D63_index.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workDir = \"/home/q506010/1_Kofax_Prod/Learning3010/\"  # Learning0910         #/1_Kofax_Mtest/ML/ frühere Massentest Daten im übergeordneten Ordner über /ML/\"\n",
    "os.chdir(workDir)\n",
    "\n",
    "searchstring = \".txt\"\n",
    "fileList = [os.path.join(dirpath, filename) for dirpath, dirname, files in os.walk(workDir) for filename in files if filename.endswith(searchstring)]\n",
    "print(\"# Files: \", len(fileList)); fileList[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Indexfiles created errors and were not read into dataframe\n",
      "Processing of 39392 files took 125 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19696, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create empty Pandas data frames to collect the content from body files and index files   \n",
    "# DF for text files with bodies\n",
    "cols = \"file_body rawBody\".upper().split()\n",
    "df_BODY = pd.DataFrame(columns=cols)\n",
    "# DF for indexfiles\n",
    "cols = \"file_index indexString\".upper().split()  \n",
    "df_INDEX = pd.DataFrame(columns=cols)\n",
    "\n",
    "# helpers\n",
    "ERRORS = []\n",
    "START_TIME = dt.datetime.now()\n",
    "COUNT = 0\n",
    "\n",
    "# Reading files from Listing\n",
    "for filepath in fileList:\n",
    "    COUNT += 1\n",
    "    fn = filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "    with open(filepath,\"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # Read Classification Infos from Indexfiles\n",
    "    if \"_index\" in fn:\n",
    "        try:\n",
    "            idx = fn.split(\"_\")[0]             # Gleichen Index verwenden wie für das Bodyfile (Emailtext)\n",
    "            df_INDEX.loc[idx,:] = filepath.split(\"/\")[-1], content \n",
    "        except:\n",
    "            ERRORS.append(filepath)\n",
    "        \n",
    "    # Read Text from Textfiles\n",
    "    else:\n",
    "        idx = fn\n",
    "        df_BODY.loc[idx,:] = filepath.split(\"/\")[-1], content \n",
    "\n",
    "df = pd.concat([df_INDEX, df_BODY], axis=1)\n",
    "\n",
    "df.INDEXSTRING = df.INDEXSTRING.str.replace(\"\\ufeff\", \"\")\n",
    "\n",
    "DURATION = dt.datetime.now() - START_TIME\n",
    "print(f\"{len(ERRORS)} Indexfiles created errors and were not read into dataframe\")\n",
    "print(f\"Processing of {COUNT} files took {DURATION.seconds} seconds\")\n",
    "\n",
    "# df.head(3)\n",
    "df.shape\n",
    "dfSIK = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all necessary information from INDEXSTRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19696 entries, 001F2D46 to 001F2D46-checkpoint\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   FILE_INDEX   19695 non-null  object\n",
      " 1   INDEXSTRING  19695 non-null  object\n",
      " 2   FILE_BODY    19696 non-null  object\n",
      " 3   RAWBODY      19696 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 769.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df = dfSIK.copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19696 entries, 001F2D46 to 001F2D46-checkpoint\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   FILE_INDEX   19695 non-null  object\n",
      " 1   INDEXSTRING  19695 non-null  string\n",
      " 2   FILE_BODY    19696 non-null  object\n",
      " 3   RAWBODY      19696 non-null  object\n",
      "dtypes: object(3), string(1)\n",
      "memory usage: 769.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfSIK.copy()\n",
    "df.INDEXSTRING = df.INDEXSTRING.astype('string')\n",
    "df.info()\n",
    "\n",
    "df.INDEXSTRING.isna().sum()\n",
    "\n",
    "# df[\"BATCHKLASSE\"] = df.INDEXSTRING.apply(lambda x: [*x.split(',')][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d0c291fcf7d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"BATCHKLASSE\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINDEXSTRING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#.replace('\"','')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# df[\"BATCHCONTENT\"]= df.INDEXSTRING.apply(lambda x: [*x.split(',')][1].replace('\"',''))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# df.BATCHKLASSE.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/python37/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-d0c291fcf7d9>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"BATCHKLASSE\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINDEXSTRING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#.replace('\"','')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# df[\"BATCHCONTENT\"]= df.INDEXSTRING.apply(lambda x: [*x.split(',')][1].replace('\"',''))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# df.BATCHKLASSE.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "df = dfSIK.copy()\n",
    "\n",
    "# df.INDEXSTRING = df.INDEXSTRING.apply(str)\n",
    "\n",
    "\n",
    "df[\"BATCHKLASSE\"] = df.INDEXSTRING.apply(lambda x: [*x.split(',')][0])  #.replace('\"','')\n",
    "# df[\"BATCHCONTENT\"]= df.INDEXSTRING.apply(lambda x: [*x.split(',')][1].replace('\"',''))\n",
    "# df.BATCHKLASSE.head()\n",
    "df[\"test1\"] = df.INDEXSTRING.apply(lambda x: [*x.split(',')])\n",
    "df[\"t2\"]    = df.INDEXSTRING.apply(lambda x: x.split(',')[0]).str.strip('\"')\n",
    "df[[\"test1\", \"t2\"]].head()\n",
    "\n",
    "\n",
    "# df.info()\n",
    "# \n",
    "# df[\"STRING\"] = df.INDEXSTRING.apply(lambda x:     [*x.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.STRING.apply(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"BATCHCONTENT\"]     = df.INDEXSTRING.apply(lambda x:     x.split(',')[2].replace('\"',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.BATCHKLASSE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns using the information provided in the indexstring\n",
    "df[\"BATCHKLASSE\"]      = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][0].replace('\"',''))\n",
    "df[\"BATCHCONTENT\"]     = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][1].replace('\"',''))\n",
    "df[\"BATCHID\"]          = df.INDEXSTRING.apply(lambda x: int([*x.split(',')][3].replace('\"','')))\n",
    "df[\"DOCID\"]            = df.INDEXSTRING.apply(lambda x: int([*x.split(',')][5].replace('\"','')))\n",
    "df[\"DOCTYPE\"]          = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][7].replace('\"',''))\n",
    "df[\"CONFIDENCE\"]       = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][9].replace('\"',''))\n",
    "# df[\"AUTOCLASS\"]        = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][19].replace('\"',''))\n",
    "df[\"PAGECOUNT\"]        = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][11]).str.strip('\"')\n",
    "df[\"DOCCOUNT\"]         = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][13]).str.strip('\"')\n",
    "df[\"INPUTCHANNEL\"]     = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][15].replace('\"',''))\n",
    "df[\"SOURCESYSTEM\"]     = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][17].replace('\"',''))\n",
    "df[\"NBR_DOCTYPES\"]     = df.CONFIDENCE.apply(lambda x: len(x))\n",
    "\n",
    "# Convert dtypes to numeric (int) and boolean (T/F)\n",
    "df.PAGECOUNT = pd.to_numeric(df.PAGECOUNT, errors='coerce').astype(\"Int64\")\n",
    "df.PAGECOUNT = df.PAGECOUNT.fillna(df.PAGECOUNT.median())\n",
    "df.DOCCOUNT = pd.to_numeric(df.DOCCOUNT, errors='coerce').astype(\"Int64\")\n",
    "df.DOCCOUNT = df.DOCCOUNT.fillna(df.DOCCOUNT.median())\n",
    "# df.AUTOCLASS  = pd.to_numeric(df.AUTOCLASS)\n",
    "# df.dropna(axis=0, subset=[\"AUTOCLASS\"], inplace=True)\n",
    "# df.AUTOCLASS  = df.AUTOCLASS.astype(\"bool\")\n",
    "\n",
    "# Split Information contained in CONFIDENCE by | Separator into list\n",
    "df.CONFIDENCE = df.CONFIDENCE.str.split(\"|\")\n",
    "\n",
    "# Drop NA records\n",
    "df = df.dropna(axis=0, subset=[\"DOCTYPE\"])\n",
    "df = df.dropna(axis=0, subset=[\"RAWBODY\"])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Explorative Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtpc = dfM.groupby(\"DOCTYPE\")[\"PAGECOUNT\"].describe(percentiles=[.8,.9,.95,.99]).sort_values(\"count\", ascending=False)\n",
    "dtpc[\"PCOUNT\"] = (dtpc[\"mean\"] * dtpc[\"count\"]).astype(\"int\")\n",
    "dtpc[\"count\"]  = dtpc[\"count\"].astype(\"int\")\n",
    "dtpc[[\"mean\", \"std\"]] = dtpc[[\"mean\", \"std\"]].round(1)\n",
    "print(\"Verteilung des Auftretens u. der Seitenzahlen der Dokumententypen\")\n",
    "dtpc.sort_values(\"PCOUNT\", ascending=False)[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "t = f\"Vert. Dokumententypen T{n} nach #Seiten\"\n",
    "_= dtpc.PCOUNT.sort_values(ascending=True)[-n:].plot(kind=\"barh\", figsize=[4,8], color=\"#5d788f\", title=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean KOFAX Data for processing on DLP platform\n",
    "\n",
    "+ Filter for records containing the relevant trainable doctypes only\n",
    "+ Blacklist-Filtering of data\n",
    "+ Cleaning & Anonymization of data\n",
    "+ Limit length of each document to a maximum length value (shortens very long documents)\n",
    "+ Select columns necessary for DLP (exclude all raw data columns!!!)\n",
    "+ Write to disk with \"\\*.pkl\" format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for records containing the relevant trainable doctypes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir  = \"/home/q506010/2_ModelData/\"\n",
    "with open(dataDir+'DocTypesTop20(Pages).txt') as f:\n",
    "    DocTypes = f.readlines()\n",
    "    f.close()\n",
    "DocTypes = sorted([dt.replace(\"\\n\", '') for dt in DocTypes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM.shape\n",
    "dfC = dfM.loc[dfM.DOCTYPE.isin(DocTypes),:].copy()\n",
    "dfC.DOCTYPE.value_counts()\n",
    "dfC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Blacklist Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC = dfM.copy()\n",
    "\n",
    "BLACKLIST = loadBlacklist()\n",
    "t0 = dt.datetime.now()\n",
    "\n",
    "# Conditional Check Column\n",
    "dfC[\"BL\"] = dfC.RAWBODY.apply(lambda x: checkBlacklist(x, BLACKLIST))\n",
    "dur = dt.datetime.now() - t0\n",
    "print(f\"Blacklist Filter took: {dur.seconds} seconds, identified {dfC.BL.sum()} records overlapping with Blacklist\")\n",
    "\n",
    "dfC = dfC.loc[dfC.BL==False, :]\n",
    "dfC.shape\n",
    "dfCSIK = dfC.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning & Anonymization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl der Zeichen\n",
    "dfC.RAWBODY.str.len().describe(percentiles=[.8, .9, .92, .95, .98, .99])\n",
    "\n",
    "# Anzahl der Wörter\n",
    "dfC.RAWBODY.str.split().str.len().describe(percentiles=[.8, .9,.95,.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC = dfCSIK.copy()\n",
    "\n",
    "time0 = dt.datetime.now()\n",
    "dfC = CleanREPLACE_KOFAX_Export(dfC)\n",
    "\n",
    "time1 = dt.datetime.now()\n",
    "dfC.BODY_CLEAN = dfC.BODY_CLEAN.apply(CleanRGX)\n",
    "\n",
    "textsize = 10_000\n",
    "dfC.BODY_CLEAN = dfC.BODY_CLEAN.apply(lambda txt: txt[:textsize])\n",
    "\n",
    "time2 = dt.datetime.now()\n",
    "dfC.BODY_CLEAN = dfC.BODY_CLEAN.apply(CleanNER)\n",
    "\n",
    "time3 = dt.datetime.now()\n",
    "d1 = time3-time2\n",
    "d2 = time3-time0\n",
    "d1.seconds; d2.seconds\n",
    "\n",
    "dfC.BODY_CLEAN.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = \"0017E075\"\n",
    "dfC.RAWBODY.loc[idx]\n",
    "len(dfC.RAWBODY.loc[idx]), len(dfC.RAWBODY.loc[idx].split())\n",
    "dfC.BODY_CLEAN.loc[idx]\n",
    "len(dfC.BODY_CLEAN.loc[idx]), len(dfC.BODY_CLEAN.loc[idx].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ = dfC.BODY_CLEAN.str.contains(\"ANONYMIZATION FAILED\")\n",
    "dfC[filter_].shape\n",
    "dfC.BL.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC.DOCTYPE.nunique()\n",
    "dfC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir  = \"/home/q506010/2_ModelData/\"\n",
    "filename = \"L3_v0.pkl\"\n",
    "\n",
    "filter_ = dfC.BODY_CLEAN.str.contains(\"ANONYMIZATION FAILED\")\n",
    "exportDF = dfC.loc[filter_==False].copy()\n",
    "exportDF = exportDF.iloc[:,[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16]]\n",
    "\n",
    "exportDF.to_pickle(dataDir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## KOFAX-Kontrollreport einlesen u. mit KOFAX-Export Dataframe matchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # file://europe.bmw.corp/winfs/SF-D-proj/_2_Internal/2018/Projects/PR-010634_DReAM_Dokumente/Auswertung%20Produktion/09%20Septepmber_Digibox_Recognition%20Time%20and%20Effectinveness/csv%20export/\n",
    "\n",
    "dfDIGIBOX = pd.read_csv('/home/q506010/1_Kofax_Prod/Recognition Time and Effectiven.csv', sep=\";\", low_memory=False)\n",
    "dfDIGIBOX.columns = [c.strip().replace(\" \",\"_\").upper().replace(\"\\xa0\",\"\") for c in dfDIGIBOX.columns.tolist()]\n",
    "\n",
    "# dtCols = [\"DOCUMENT_ARRIVAL_DEAS\",\n",
    "#           \"DOCUMENT_SENT_TO_MANUALCLASSIFICATION\",\n",
    "#           \"DOCUMENT_MANUALCLASSIFICATION\",\n",
    "#           \"DOCUMENT_SENT_TO_MANUAL_EXTRACTION\",\n",
    "#           \"DOCUMENT_MANUAL_EXTRACTION\",\n",
    "#           \"DOCUMENT_SENT_TO_DMS\"]\n",
    "# for col in dtCols:\n",
    "#     dfDIGIBOX[col] = pd.to_datetime(dfDIGIBOX[col])\n",
    "\n",
    "# dfDIGIBOX = dfDIGIBOX.iloc[:,[0,1,2,3,5,15,16,17,19]]\n",
    "dfDIGIBOX.info()\n",
    "dfDIGIBOX.head(2)\n",
    "\n",
    "dfMClass = dfDIGIBOX.loc[dfDIGIBOX.AUTOMATISCH_KLASSIFIZIERT==0,:].copy()\n",
    "print(f\"Nicht automatisch klassifiert {dfMClass.shape[0]} Dokumente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM = df.merge(dfDIGIBOX, how=\"left\", left_on=\"BATCHID\", right_on=\"STAPEL_ID\", )  # left oder inner Merge\n",
    "(dfM.STAPEL_ID == dfM.BATCHID).sum()\n",
    "dfM.info() \n",
    "dfM.AUTOMATISCH_KLASSIFIZIERT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parkplatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {1, 2, 3, 4, 5}\n",
    "b = { 3, 5, 6, 7, 8}\n",
    "c = b.intersection(a)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested List Comprehension\n",
    "non_flat = [ [1,2,3], [4,5,6], [7,8] ]\n",
    "[y for x in non_flat for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Dataframe Grundgerüst zum \"Einsammeln der Dokumenten-Infos\"   \n",
    "# Bodyfiles\n",
    "cols = \"file_body rawBody\".upper().split()\n",
    "df_BODY = pd.DataFrame(columns=cols)\n",
    "# Indexfiles\n",
    "cols = \"file_index indexString batchID docID docType confidence pageCount docCount inputChannel sourceSystem\".upper().split()   # rawBody\n",
    "df_INDEX = pd.DataFrame(columns=cols)\n",
    "\n",
    "ERRORS = []\n",
    "# Reading files from Listing\n",
    "for file in fileList: #[:1000]:\n",
    "    fn = file.split(\"/\")[-1].split(\".\")[0]\n",
    "    # print(fn)    \n",
    "    # Read Classification Infos from Indexfiles\n",
    "    if \"_index\" in fn:\n",
    "        with open(file,\"r\", encoding=\"utf-8\") as f:  #encoding=\"utf-8\"\n",
    "            content = f.read()\n",
    "            \n",
    "        try:\n",
    "            batchID, docID, docType, confidence, pageCount, documentCount, inputChannel, sourceSystem = readIndexFileInfo(content)\n",
    "            idx = fn.split(\"_\")[0]             # Gleichen Index verwenden wie für das Bodyfile (Emailtext)\n",
    "            df_INDEX.loc[idx,:] = file.split(\"/\")[-1], content, batchID, docID, docType, confidence, pageCount, documentCount, inputChannel, sourceSystem\n",
    "\n",
    "        except:\n",
    "            ERRORS.append(file)\n",
    "    \n",
    "    \n",
    "    # Read Emailtext from Textfiles\n",
    "    else:\n",
    "        with open(file,\"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        idx = fn\n",
    "        df_BODY.loc[idx,:] = file.split(\"/\")[-1], content \n",
    "\n",
    "df = pd.concat([df_INDEX, df_BODY], axis=1)\n",
    "\n",
    "# Ergänzende Spalten bauen\n",
    "df.CONFIDENCE = df.CONFIDENCE.str.split(\"|\")\n",
    "\n",
    "# df[\"BATCHKLASSE\"] = df.INDEXSTRING.apply(lambda text: text.split(\",\")[0]).str.replace('\"', '').str.lstrip(\"\\ufeff\")\n",
    "# df.INDEXSTRING = df.INDEXSTRING.str.replace(\"\\ufeff\", \"\")\n",
    "# df[\"BATCHKLASSE\"] = df.INDEXSTRING.str.split(\",\")\n",
    "# df.BATCHKLASSE = df.BATCHKLASSE.apply(lambda liste: liste[0].replace('\"', ''))\n",
    "\n",
    "print(f\"{len(ERRORS)} Indexfiles created errors and were not read into dataframe\")\n",
    "pd.set_option('display.max_colwidth', 30)\n",
    "df.head(); df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readIndexFileInfo(txt):\n",
    "    \n",
    "    # Definition of RGX-objects to find dedicated infos in string\n",
    "    RGX_batchID       = re.compile( r'\"{Batch ID}\",\"(\\d{6})' )                                  # Capture 6-Digit Batch ID value\n",
    "    RGX_docID         = re.compile( r'\"{Document ID}\",\"(\\d{7})' )                               # Capture 7-Digit Document ID value\n",
    "    RGX_docType       = re.compile( r'\"DocumentType\",\"(\\w+)\"' )                                 # Capture Document Type\n",
    "    RGX_confidence    = re.compile( r'\"ClassificationResultWithConfiden\",\"(.*)\",\"PageCount\"' )  # Capture ALL found Document IDs with Confidence Scores\n",
    "    RGX_pageCount     = re.compile( r'\"PageCount\",\"(\\d+)\"' )                                    # Capture Page Count \n",
    "    RGX_documentCount = re.compile( r'\"\\{?Document\\s?Count\\}?\",\"(\\d+)\"' )                       # Capture Document Count\n",
    "    RGX_inputChannel  = re.compile( r'\"{\\$InputChannel}\",\"(\\w+)\"' )                             # Capture Input Channel\n",
    "    RGX_sourceSystem  = re.compile( r'\"{\\$sourceSystem}\",\"(\\w+)\"' )                             # Capture Source System\n",
    "    \n",
    "    # Perform search on above RGX-objects and fill variables with results\n",
    "    MO_BatchID        = RGX_batchID.search(content)        # MO_ := Match-Object\n",
    "    batchID           = int(MO_BatchID.group(1))\n",
    "    \n",
    "    MO_DocID          = RGX_docID.search(content)\n",
    "    docID             = int(MO_DocID.group(1))\n",
    "    \n",
    "    MO_DocType        = RGX_docType.search(content)\n",
    "    docType           = MO_DocType.group(1)\n",
    "        \n",
    "    MO_Confidence     = RGX_confidence.search(content)\n",
    "    confidence        = MO_Confidence.group(1)\n",
    "    \n",
    "    MO_PageCount      = RGX_pageCount.search(content)\n",
    "    pageCount         = int(MO_PageCount.group(1))\n",
    "    \n",
    "    MO_DocumentCount  = RGX_documentCount.search(content)\n",
    "    documentCount     = int(MO_DocumentCount.group(1))\n",
    "     \n",
    "    MO_InputChannel   = RGX_inputChannel.search(content)\n",
    "    inputChannel      = MO_InputChannel.group(1)\n",
    "    \n",
    "    MO_SourceSystem   = RGX_sourceSystem.search(content)\n",
    "    sourceSystem      = MO_SourceSystem.group(1)\n",
    "    \n",
    "    return batchID, docID, docType, confidence, pageCount, documentCount, inputChannel, sourceSystem\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auspacker(inputstring):\n",
    "    \n",
    "    result = []\n",
    "    for tl in inputstring.split(\"|\"):\n",
    "        score = float(tl.split(\";\")[1])\n",
    "        dtype = tl.split(\";\")[0]\n",
    "        code = tl.split(\";\")[2]\n",
    "        result.append((score, dtype, code))\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auspacker(liste):\n",
    "    L1 = [*liste.split(\"|\")]\n",
    "    L2 = [[float(tl.split(\";\")[1]), tl.split(\";\")[0], tl.split(\";\")[2]] for tl in L1]\n",
    "    return L2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(10,10))\n",
    "\n",
    "d1 = dtpc.PCOUNT.sort_values(ascending=True)[-n:]\n",
    "d2 = dfM.DOCTYPE.value_counts()[:n].sort_values(ascending=True)\n",
    "\n",
    "t1 = f\"Vert. Dokumententypen T{n} nach #Seiten\"\n",
    "t2 = f\"... nach Häufigkeit\"\n",
    "\n",
    "ax1.barh(d1.index, d1, color=\"#5d788f\")\n",
    "ax1.set_title(t1)\n",
    "\n",
    "ax2.barh(d2.index, d2, color=\"g\",)\n",
    "ax2.set_title(t2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select necessary columns\n",
    "dfC = dfM.iloc[:, [3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15] ].copy()\n",
    "dfC.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
