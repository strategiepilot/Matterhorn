{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ KOFAX EXPORT DATA (DREAM)\n",
    "\n",
    "Author:   Andreas Barth, SF6-S-OG  \n",
    "Version:  4, 12.10.2020  \n",
    "\n",
    "Script to read the email informations (doctypes, body, subject, batchID, pagecount, etc.) out of Export Scripts (File Structure) provided by KOFAX export.\n",
    "\n",
    "HSDAP ONLY! ... (Raw Data is not anonymized yet)\n",
    "\n",
    "\n",
    "Steps:\n",
    "  \n",
    "0. Imports & Functions\n",
    "1. Read RawData into Dataframe\n",
    "2. Transform RawData: Blacklist Check, Cleaning, Anonymization & Preprocessing\n",
    "3. Save Cleaned Data to Disk (Pickle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os, re, string, sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==================== BMW-Bank Funktionen ============================\n",
    "workDir = os.getcwd()\n",
    "os.chdir('/mnt/hsdapnas01/shared/sf6/0_Tools')\n",
    "from BmwBankTools.cleanEmails import * \n",
    "os.chdir(workDir)\n",
    "\n",
    "# ==========================================================\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "np.random.seed(4711)\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "plt.style.use('ggplot')\n",
    "#===========================================================\n",
    "print(sys.version, sys.getdefaultencoding())\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 1. Read KOFAX exported data from file structure on disk (HSDAP)\n",
    "#### 1.1 Create Listing with filepath of each file to be read into system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportDir = '/mnt/hsdapnas01/shared/sf6/1_TrainingData/'\n",
    "dataDir   = '/mnt/hsdapnas01/shared/sf6/1_RawData/'\n",
    "DirectoryList = [f for f in os.listdir(dataDir) if \"Learning\" in f and \"zip\" not in f]\n",
    "print(DirectoryList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetDir = DirectoryList[0]                  # Choose directory with rawdata that is to be read into dataframes\n",
    "targetDir = os.path.join(dataDir, targetDir)\n",
    "searchstring = \".txt\"\n",
    "fileList = [os.path.join(dirpath, filename) for dirpath, dirname, files in os.walk(targetDir) for filename in files if filename.endswith(searchstring)]\n",
    "print(\"# Files: \", len(fileList)); fileList[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Read RawData into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BITTE NUR ZUM TESTEN BENUTZEN ... SONST RAUSNEHMEN ODER ÜBERSPRINGEN\n",
    "fileList = fileList[:10_000]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 empty Pandas data frames: 1 to collect the content from body files and 1 for the index files   \n",
    "# DF for text files with bodies\n",
    "cols = \"file_body rawBody\".upper().split()\n",
    "df_BODY = pd.DataFrame(columns=cols)\n",
    "# DF for indexfiles\n",
    "cols = \"file_index indexString\".upper().split()  \n",
    "df_INDEX = pd.DataFrame(columns=cols)\n",
    "\n",
    "# helpers\n",
    "ERRORS = []\n",
    "START_TIME = dt.datetime.now()\n",
    "COUNT = 0\n",
    "\n",
    "# Reading files from Listing\n",
    "for filepath in tqdm(fileList, desc=\"Collecting Documents\"):\n",
    "    COUNT += 1\n",
    "    fn = filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "    with open(filepath,\"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # Read Classification Infos from Indexfiles\n",
    "    if \"_index\" in fn:\n",
    "        try:\n",
    "            idx = fn.split(\"_\")[0]             # Gleichen Index verwenden wie für das Bodyfile (Emailtext)\n",
    "            df_INDEX.loc[idx,:] = filepath.split(\"/\")[-1], content \n",
    "        except:\n",
    "            ERRORS.append(filepath)\n",
    "        \n",
    "    # Read Text from Textfiles\n",
    "    else:\n",
    "        idx = fn\n",
    "        df_BODY.loc[idx,:] = filepath.split(\"/\")[-1], content \n",
    "\n",
    "# Concatenate the 2 dataframes to one \n",
    "df = pd.concat([df_INDEX, df_BODY], axis=1)\n",
    "df.dropna(axis=0, subset=[\"INDEXSTRING\",\"RAWBODY\"], inplace=True)\n",
    "\n",
    "df.INDEXSTRING = df.INDEXSTRING.str.replace(\"\\ufeff\", \"\")\n",
    "\n",
    "DURATION = dt.datetime.now() - START_TIME\n",
    "print(f\"{len(ERRORS)} Indexfiles created errors and were not read into dataframe\")\n",
    "print(f\"Processing of {COUNT} files took {DURATION.seconds} seconds\")\n",
    "\n",
    "df.shape; df.head(3)\n",
    "dfSIK = df.copy()\n",
    "del df_INDEX, df_BODY\n",
    "df.RAWBODY.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Filter for records that contain an AutoClassifiction-Tag in the Indexstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many?\n",
    "print(\"# of Records: \", df.INDEXSTRING.str.contains(\"AutoClassificationConfidence\").sum())\n",
    "print(\"% of Records with AutoClass Info: \",df.INDEXSTRING.str.contains(\"AutoClassificationConfidence\").mean().round(3))\n",
    "\n",
    "# Filter dataframe for records with AutoClassification Tag only\n",
    "df = df.loc[df.INDEXSTRING.str.contains(\"AutoClassificationConfidence\")==True,:].copy()\n",
    "print(\"Filtered Dataframe, Shape: \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Read all necessary information from INDEXSTRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfSIK.copy()\n",
    "\n",
    "# Create columns using the information provided in the indexstring\n",
    "df[\"BATCHKLASSE\"]      = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][0].replace('\"',''))\n",
    "df[\"BATCHCONTENT\"]     = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][1].replace('\"',''))\n",
    "df[\"BATCHID\"]          = df.INDEXSTRING.apply(lambda x: int([*x.split(',')][3].replace('\"','')))\n",
    "df[\"DOCID\"]            = df.INDEXSTRING.apply(lambda x: int([*x.split(',')][5].replace('\"','')))\n",
    "df[\"DOCTYPE\"]          = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][7].replace('\"',''))\n",
    "df[\"CONFIDENCE\"]       = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][9].replace('\"',''))\n",
    "df[\"AUTOCLASS\"]        = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][19].replace('\"',''))\n",
    "df[\"PAGECOUNT\"]        = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][11]).str.strip('\"')\n",
    "df[\"DOCCOUNT\"]         = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][13]).str.strip('\"')\n",
    "df[\"INPUTCHANNEL\"]     = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][15].replace('\"',''))\n",
    "df[\"SOURCESYSTEM\"]     = df.INDEXSTRING.apply(lambda x:     [*x.split(',')][17].replace('\"',''))\n",
    "\n",
    "# Convert dtypes to numeric (int) and boolean (T/F)\n",
    "df.PAGECOUNT  = pd.to_numeric(df.PAGECOUNT, errors='coerce').astype(\"Int64\")\n",
    "df.PAGECOUNT  = df.PAGECOUNT.fillna(df.PAGECOUNT.median())\n",
    "df.DOCCOUNT   = pd.to_numeric(df.DOCCOUNT, errors='coerce').astype(\"Int64\")\n",
    "df.DOCCOUNT   = df.DOCCOUNT.fillna(df.DOCCOUNT.median())\n",
    "df.AUTOCLASS  = pd.to_numeric(df.AUTOCLASS)\n",
    "print(\"NA Values auf AUTOCLASS: \", df.AUTOCLASS.isna().sum())\n",
    "df.dropna(axis=0, subset=[\"AUTOCLASS\"], inplace=True)\n",
    "df.AUTOCLASS  = df.AUTOCLASS.astype(\"bool\")\n",
    "\n",
    "# Split Information contained in CONFIDENCE by | Separator into list\n",
    "df.CONFIDENCE = df.CONFIDENCE.str.split(\"|\")\n",
    "df[\"NBR_DOCTYPES\"] = df.CONFIDENCE.apply(lambda x: len(x))\n",
    "\n",
    "# Drop NA records\n",
    "print(\"NA Values auf DOCTYPE: \", df.DOCTYPE.isna().sum())\n",
    "print(\"NA Values auf BODY: \", df.RAWBODY.isna().sum())\n",
    "df = df.dropna(axis=0, subset=[\"DOCTYPE\", \"RAWBODY\"])\n",
    "print(\"Dataframe, Shape: \", df.shape)\n",
    "\n",
    "dfSIK2 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Optional Save RawData before further Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FILENAME.pkl\"\n",
    "df.to_pickle(dataDir+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.6 Optional: Explorative Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.PAGECOUNT.sum()                                     # number of total pages\n",
    "df.DOCTYPE.nunique()\n",
    "df.AUTOCLASS.value_counts(normalize=True).round(3)     # records with AutoClass Info contained\n",
    "df.AUTOCLASS.value_counts(normalize=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. Clean KOFAX Data for processing on DLP platform\n",
    "\n",
    "+ Filter for AUTOCLASSIFICATION == FALSE\n",
    "+ Blacklist-Filtering of data\n",
    "+ Cleaning & Anonymization of data\n",
    "+ Limit length of each document to a maximum length value (shortens very long documents)\n",
    "+ Select columns necessary for DLP (exclude all raw data columns!!!)\n",
    "+ Write to disk with \"\\*.pkl\" format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Filter for records with tag \"AUTOCLASSIFICATION\" set to FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM = df.loc[df.AUTOCLASS==False,:].copy()    #Filter for records with tag \"AUTOCLASSIFICATION\" set to FALSE\n",
    "print(f\"Anz. Dokumente die nicht autom. klassifiziert werden: # {dfM.shape[0]}\")\n",
    "print(f\"Anz. Seiten, die nicht autom. klassifiziert werden: # {dfM.PAGECOUNT.sum()}\")\n",
    "dfM.shape\n",
    "dfM.DOCTYPE.nunique()\n",
    "dfM.PAGECOUNT.sum()\n",
    "dfM.AUTOCLASS.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Blacklist Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"Matching with Blacklist\")\n",
    "\n",
    "dfC = dfM.copy()\n",
    "\n",
    "t0 = dt.datetime.now()\n",
    "blackListFile = '/mnt/hsdapnas01/shared/sf6/0_Tools/BlacklistEmail.pkl' \n",
    "BLACKLIST = loadBlacklist(blackListFile)\n",
    "print(f\"Blacklist with {len(BLACKLIST)} records loaded\")\n",
    "print(\"Checking against Blacklist ...\")\n",
    "\n",
    "# Conditional Check Column\n",
    "# dfC[\"BL\"] = dfC.RAWBODY.apply(lambda x: checkBlacklist(x, BLACKLIST))\n",
    "dfC[\"BL\"] = dfC.RAWBODY.progress_apply(lambda x: checkBlacklist(x, BLACKLIST))\n",
    "\n",
    "dur = dt.datetime.now() - t0\n",
    "print(f\"Blacklist Filter took: {dur.seconds} seconds, identified {dfC.BL.sum()} records overlapping with Blacklist\")\n",
    "\n",
    "dfC = dfC.loc[dfC.BL==False, :]\n",
    "dfC.shape\n",
    "dfCSIK = dfC.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2b Optional Save Blacklist-Checked Data before further Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FILENAME.pkl\"\n",
    "dfC.to_pickle(dataDir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC.DOCTYPE.nunique()\n",
    "dfC.AUTOCLASS.nunique()\n",
    "dfC.PAGECOUNT.sum()\n",
    "dfC.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Cleaning & Anonymization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "time0 = dt.datetime.now()\n",
    "dfC = CleanREPLACE_KOFAX_Export(dfC)\n",
    "\n",
    "# Cleaning Stufe 2 (Regex Rules)\n",
    "tqdm.pandas(desc=\"Run Regex Rules for Anonymization & Cleaning\")\n",
    "time1 = dt.datetime.now()\n",
    "dfC.BODY_CLEAN = dfC.BODY_CLEAN.progress_apply(CleanRGX)\n",
    "\n",
    "# Setting Maximum Text Length before Spacy NER Function is applied\n",
    "textsize = 10_000\n",
    "tqdm.pandas(desc=f\"Trimming all Documents to max length of {textsize} characters\")\n",
    "dfC.BODY_CLEAN = dfC.BODY_CLEAN.progress_apply(lambda txt: txt[:textsize])\n",
    "\n",
    "time2 = dt.datetime.now()\n",
    "tqdm.pandas(desc=\"NER Detection & Anonymization\")\n",
    "dfC.BODY_CLEAN = dfC.BODY_CLEAN.progress_apply(CleanNER)\n",
    "\n",
    "time3 = dt.datetime.now()\n",
    "d1 = time3-time2\n",
    "d2 = time3-time0\n",
    "d1.seconds; d2.seconds\n",
    "\n",
    "dfC.shape\n",
    "dfC.DOCTYPE.nunique()\n",
    "dfC.BODY_CLEAN.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Save Data to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"LXX_v0.pkl\"\n",
    "exportDir = '/mnt/hsdapnas01/shared/sf6/1_TrainingData/' \n",
    "\n",
    "filter_  = dfC.BODY_CLEAN.str.contains(\"ANONYMIZATION FAILED\")\n",
    "exportDF = dfC.loc[filter_==False].copy()\n",
    "exportDF = exportDF.iloc[:,[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17]]\n",
    "\n",
    "exportDF.to_pickle(exportDir+filename)\n",
    "exportDF.shape\n",
    "exportDF.DOCTYPE.nunique()\n",
    "exportDF.PAGECOUNT.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportDF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parkplatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
